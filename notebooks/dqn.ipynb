{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "Задаем структуру аппроксимации $Q^\\theta$, начальные вектор параметров $\\theta$, вероятность исследования среды $\\varepsilon = 1$.\n",
    "\n",
    "Для каждого эпизода $k$ делаем:\n",
    "\n",
    "Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие $A_t \\sim \\pi(\\cdot|S_t)$, где $\\pi = \\varepsilon\\text{-greedy}(Q^\\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \\rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем целевые значения\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "r_i, &\\text{ если } s'_i\\text{ -терминальное},\\\\[0.0cm]\n",
    " r_i + \\gamma \\max\\limits_{a'} Q^\\theta(s'_i,a'), &\\text{ иначе}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "функцию потерь $Loss(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2$\n",
    "и обновляем вектор параметров\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss(\\theta)\n",
    "$$\n",
    "\n",
    "- Уменьшаем $\\varepsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidder_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_dim, hidder_size)\n",
    "        self.linear_2 = nn.Linear(hidder_size, hidder_size)\n",
    "        self.linear_3 = nn.Linear(hidder_size, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.linear_1(input)\n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.linear_2(hidden)\n",
    "        hidden = self.activation(hidden)\n",
    "        output = self.linear_3(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_n, epsilon_decrease, gamma=0.99, batch_size=128, lr=1e-3, epsilon_min=1e-2):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        self.q_model = Network(self.state_dim, self.action_n)\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon = 1\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q_values = self.q_model(torch.FloatTensor(state)).data.numpy()\n",
    "        max_action = np.argmax(q_values)\n",
    "        probs = np.ones(self.action_n) * self.epsilon / self.action_n\n",
    "        probs[max_action] += 1 - self.epsilon\n",
    "        return np.random.choice(np.arange(self.action_n), p=probs)\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.tensor, zip(*batch))\n",
    "            \n",
    "            targets = rewards + (1 - dones) * self.gamma * torch.max(self.q_model(next_states), dim=1).values\n",
    "            q_values = self.q_model(states)[torch.arange(self.batch_size), actions]\n",
    "            loss = torch.mean((q_values - targets) ** 2)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.epsilon = max(self.epsilon - self.epsilon_decrease, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory: 0, total_reward 11.0\n",
      "trajectory: 1, total_reward 36.0\n",
      "trajectory: 2, total_reward 38.0\n",
      "trajectory: 3, total_reward 20.0\n",
      "trajectory: 4, total_reward 16.0\n",
      "trajectory: 5, total_reward 28.0\n",
      "trajectory: 6, total_reward 29.0\n",
      "trajectory: 7, total_reward 22.0\n",
      "trajectory: 8, total_reward 25.0\n",
      "trajectory: 9, total_reward 15.0\n",
      "trajectory: 10, total_reward 20.0\n",
      "trajectory: 11, total_reward 24.0\n",
      "trajectory: 12, total_reward 34.0\n",
      "trajectory: 13, total_reward 15.0\n",
      "trajectory: 14, total_reward 25.0\n",
      "trajectory: 15, total_reward 12.0\n",
      "trajectory: 16, total_reward 22.0\n",
      "trajectory: 17, total_reward 16.0\n",
      "trajectory: 18, total_reward 13.0\n",
      "trajectory: 19, total_reward 17.0\n",
      "trajectory: 20, total_reward 25.0\n",
      "trajectory: 21, total_reward 74.0\n",
      "trajectory: 22, total_reward 11.0\n",
      "trajectory: 23, total_reward 16.0\n",
      "trajectory: 24, total_reward 12.0\n",
      "trajectory: 25, total_reward 138.0\n",
      "trajectory: 26, total_reward 29.0\n",
      "trajectory: 27, total_reward 11.0\n",
      "trajectory: 28, total_reward 14.0\n",
      "trajectory: 29, total_reward 16.0\n",
      "trajectory: 30, total_reward 13.0\n",
      "trajectory: 31, total_reward 23.0\n",
      "trajectory: 32, total_reward 17.0\n",
      "trajectory: 33, total_reward 35.0\n",
      "trajectory: 34, total_reward 18.0\n",
      "trajectory: 35, total_reward 29.0\n",
      "trajectory: 36, total_reward 11.0\n",
      "trajectory: 37, total_reward 12.0\n",
      "trajectory: 38, total_reward 10.0\n",
      "trajectory: 39, total_reward 30.0\n",
      "trajectory: 40, total_reward 9.0\n",
      "trajectory: 41, total_reward 23.0\n",
      "trajectory: 42, total_reward 11.0\n",
      "trajectory: 43, total_reward 39.0\n",
      "trajectory: 44, total_reward 41.0\n",
      "trajectory: 45, total_reward 14.0\n",
      "trajectory: 46, total_reward 34.0\n",
      "trajectory: 47, total_reward 45.0\n",
      "trajectory: 48, total_reward 40.0\n",
      "trajectory: 49, total_reward 48.0\n",
      "trajectory: 50, total_reward 58.0\n",
      "trajectory: 51, total_reward 43.0\n",
      "trajectory: 52, total_reward 34.0\n",
      "trajectory: 53, total_reward 34.0\n",
      "trajectory: 54, total_reward 24.0\n",
      "trajectory: 55, total_reward 33.0\n",
      "trajectory: 56, total_reward 21.0\n",
      "trajectory: 57, total_reward 19.0\n",
      "trajectory: 58, total_reward 22.0\n",
      "trajectory: 59, total_reward 13.0\n",
      "trajectory: 60, total_reward 36.0\n",
      "trajectory: 61, total_reward 12.0\n",
      "trajectory: 62, total_reward 31.0\n",
      "trajectory: 63, total_reward 18.0\n",
      "trajectory: 64, total_reward 16.0\n",
      "trajectory: 65, total_reward 11.0\n",
      "trajectory: 66, total_reward 21.0\n",
      "trajectory: 67, total_reward 16.0\n",
      "trajectory: 68, total_reward 21.0\n",
      "trajectory: 69, total_reward 21.0\n",
      "trajectory: 70, total_reward 29.0\n",
      "trajectory: 71, total_reward 43.0\n",
      "trajectory: 72, total_reward 14.0\n",
      "trajectory: 73, total_reward 15.0\n",
      "trajectory: 74, total_reward 36.0\n",
      "trajectory: 75, total_reward 47.0\n",
      "trajectory: 76, total_reward 57.0\n",
      "trajectory: 77, total_reward 11.0\n",
      "trajectory: 78, total_reward 40.0\n",
      "trajectory: 79, total_reward 16.0\n",
      "trajectory: 80, total_reward 25.0\n",
      "trajectory: 81, total_reward 59.0\n",
      "trajectory: 82, total_reward 49.0\n",
      "trajectory: 83, total_reward 22.0\n",
      "trajectory: 84, total_reward 20.0\n",
      "trajectory: 85, total_reward 22.0\n",
      "trajectory: 86, total_reward 22.0\n",
      "trajectory: 87, total_reward 70.0\n",
      "trajectory: 88, total_reward 11.0\n",
      "trajectory: 89, total_reward 15.0\n",
      "trajectory: 90, total_reward 65.0\n",
      "trajectory: 91, total_reward 26.0\n",
      "trajectory: 92, total_reward 42.0\n",
      "trajectory: 93, total_reward 19.0\n",
      "trajectory: 94, total_reward 15.0\n",
      "trajectory: 95, total_reward 110.0\n",
      "trajectory: 96, total_reward 23.0\n",
      "trajectory: 97, total_reward 85.0\n",
      "trajectory: 98, total_reward 49.0\n",
      "trajectory: 99, total_reward 44.0\n",
      "trajectory: 100, total_reward 20.0\n",
      "trajectory: 101, total_reward 30.0\n",
      "trajectory: 102, total_reward 47.0\n",
      "trajectory: 103, total_reward 50.0\n",
      "trajectory: 104, total_reward 18.0\n",
      "trajectory: 105, total_reward 15.0\n",
      "trajectory: 106, total_reward 56.0\n",
      "trajectory: 107, total_reward 62.0\n",
      "trajectory: 108, total_reward 16.0\n",
      "trajectory: 109, total_reward 46.0\n",
      "trajectory: 110, total_reward 13.0\n",
      "trajectory: 111, total_reward 66.0\n",
      "trajectory: 112, total_reward 86.0\n",
      "trajectory: 113, total_reward 11.0\n",
      "trajectory: 114, total_reward 74.0\n",
      "trajectory: 115, total_reward 94.0\n",
      "trajectory: 116, total_reward 20.0\n",
      "trajectory: 117, total_reward 80.0\n",
      "trajectory: 118, total_reward 65.0\n",
      "trajectory: 119, total_reward 64.0\n",
      "trajectory: 120, total_reward 66.0\n",
      "trajectory: 121, total_reward 41.0\n",
      "trajectory: 122, total_reward 21.0\n",
      "trajectory: 123, total_reward 14.0\n",
      "trajectory: 124, total_reward 161.0\n",
      "trajectory: 125, total_reward 29.0\n",
      "trajectory: 126, total_reward 16.0\n",
      "trajectory: 127, total_reward 79.0\n",
      "trajectory: 128, total_reward 52.0\n",
      "trajectory: 129, total_reward 29.0\n",
      "trajectory: 130, total_reward 197.0\n",
      "trajectory: 131, total_reward 180.0\n",
      "trajectory: 132, total_reward 208.0\n",
      "trajectory: 133, total_reward 13.0\n",
      "trajectory: 134, total_reward 56.0\n",
      "trajectory: 135, total_reward 190.0\n",
      "trajectory: 136, total_reward 47.0\n",
      "trajectory: 137, total_reward 28.0\n",
      "trajectory: 138, total_reward 83.0\n",
      "trajectory: 139, total_reward 21.0\n",
      "trajectory: 140, total_reward 109.0\n",
      "trajectory: 141, total_reward 135.0\n",
      "trajectory: 142, total_reward 64.0\n",
      "trajectory: 143, total_reward 36.0\n",
      "trajectory: 144, total_reward 97.0\n",
      "trajectory: 145, total_reward 104.0\n",
      "trajectory: 146, total_reward 90.0\n",
      "trajectory: 147, total_reward 105.0\n",
      "trajectory: 148, total_reward 69.0\n",
      "trajectory: 149, total_reward 93.0\n",
      "trajectory: 150, total_reward 59.0\n",
      "trajectory: 151, total_reward 118.0\n",
      "trajectory: 152, total_reward 94.0\n",
      "trajectory: 153, total_reward 58.0\n",
      "trajectory: 154, total_reward 101.0\n",
      "trajectory: 155, total_reward 202.0\n",
      "trajectory: 156, total_reward 130.0\n",
      "trajectory: 157, total_reward 109.0\n",
      "trajectory: 158, total_reward 102.0\n",
      "trajectory: 159, total_reward 183.0\n",
      "trajectory: 160, total_reward 116.0\n",
      "trajectory: 161, total_reward 104.0\n",
      "trajectory: 162, total_reward 159.0\n",
      "trajectory: 163, total_reward 265.0\n",
      "trajectory: 164, total_reward 261.0\n",
      "trajectory: 165, total_reward 171.0\n",
      "trajectory: 166, total_reward 500.0\n",
      "trajectory: 167, total_reward 500.0\n",
      "trajectory: 168, total_reward 226.0\n",
      "trajectory: 169, total_reward 236.0\n",
      "trajectory: 170, total_reward 171.0\n",
      "trajectory: 171, total_reward 451.0\n",
      "trajectory: 172, total_reward 80.0\n",
      "trajectory: 173, total_reward 260.0\n",
      "trajectory: 174, total_reward 232.0\n",
      "trajectory: 175, total_reward 152.0\n",
      "trajectory: 176, total_reward 168.0\n",
      "trajectory: 177, total_reward 241.0\n",
      "trajectory: 178, total_reward 270.0\n",
      "trajectory: 179, total_reward 285.0\n",
      "trajectory: 180, total_reward 359.0\n",
      "trajectory: 181, total_reward 193.0\n",
      "trajectory: 182, total_reward 103.0\n",
      "trajectory: 183, total_reward 133.0\n",
      "trajectory: 184, total_reward 274.0\n",
      "trajectory: 185, total_reward 287.0\n",
      "trajectory: 186, total_reward 306.0\n",
      "trajectory: 187, total_reward 265.0\n",
      "trajectory: 188, total_reward 303.0\n",
      "trajectory: 189, total_reward 405.0\n",
      "trajectory: 190, total_reward 207.0\n",
      "trajectory: 191, total_reward 360.0\n",
      "trajectory: 192, total_reward 389.0\n",
      "trajectory: 193, total_reward 231.0\n",
      "trajectory: 194, total_reward 290.0\n",
      "trajectory: 195, total_reward 306.0\n",
      "trajectory: 196, total_reward 417.0\n",
      "trajectory: 197, total_reward 296.0\n",
      "trajectory: 198, total_reward 274.0\n",
      "trajectory: 199, total_reward 306.0\n",
      "trajectory: 200, total_reward 264.0\n",
      "trajectory: 201, total_reward 385.0\n",
      "trajectory: 202, total_reward 268.0\n",
      "trajectory: 203, total_reward 500.0\n",
      "trajectory: 204, total_reward 264.0\n",
      "trajectory: 205, total_reward 362.0\n",
      "trajectory: 206, total_reward 200.0\n",
      "trajectory: 207, total_reward 324.0\n",
      "trajectory: 208, total_reward 434.0\n",
      "trajectory: 209, total_reward 461.0\n",
      "trajectory: 210, total_reward 500.0\n",
      "trajectory: 211, total_reward 450.0\n",
      "trajectory: 212, total_reward 246.0\n",
      "trajectory: 213, total_reward 325.0\n",
      "trajectory: 214, total_reward 337.0\n",
      "trajectory: 215, total_reward 341.0\n",
      "trajectory: 216, total_reward 426.0\n",
      "trajectory: 217, total_reward 316.0\n",
      "trajectory: 218, total_reward 362.0\n",
      "trajectory: 219, total_reward 290.0\n",
      "trajectory: 220, total_reward 500.0\n",
      "trajectory: 221, total_reward 347.0\n",
      "trajectory: 222, total_reward 234.0\n",
      "trajectory: 223, total_reward 313.0\n",
      "trajectory: 224, total_reward 287.0\n",
      "trajectory: 225, total_reward 382.0\n",
      "trajectory: 226, total_reward 298.0\n",
      "trajectory: 227, total_reward 500.0\n",
      "trajectory: 228, total_reward 412.0\n",
      "trajectory: 229, total_reward 320.0\n",
      "trajectory: 230, total_reward 256.0\n",
      "trajectory: 231, total_reward 410.0\n",
      "trajectory: 232, total_reward 306.0\n",
      "trajectory: 233, total_reward 377.0\n",
      "trajectory: 234, total_reward 343.0\n",
      "trajectory: 235, total_reward 370.0\n",
      "trajectory: 236, total_reward 414.0\n",
      "trajectory: 237, total_reward 331.0\n",
      "trajectory: 238, total_reward 500.0\n",
      "trajectory: 239, total_reward 375.0\n",
      "trajectory: 240, total_reward 314.0\n",
      "trajectory: 241, total_reward 375.0\n",
      "trajectory: 242, total_reward 306.0\n",
      "trajectory: 243, total_reward 389.0\n",
      "trajectory: 244, total_reward 454.0\n",
      "trajectory: 245, total_reward 343.0\n",
      "trajectory: 246, total_reward 271.0\n",
      "trajectory: 247, total_reward 349.0\n",
      "trajectory: 248, total_reward 482.0\n",
      "trajectory: 249, total_reward 471.0\n",
      "trajectory: 250, total_reward 302.0\n",
      "trajectory: 251, total_reward 500.0\n",
      "trajectory: 252, total_reward 386.0\n",
      "trajectory: 253, total_reward 500.0\n",
      "trajectory: 254, total_reward 443.0\n",
      "trajectory: 255, total_reward 339.0\n",
      "trajectory: 256, total_reward 500.0\n",
      "trajectory: 257, total_reward 500.0\n",
      "trajectory: 258, total_reward 395.0\n",
      "trajectory: 259, total_reward 431.0\n",
      "trajectory: 260, total_reward 407.0\n",
      "trajectory: 261, total_reward 345.0\n",
      "trajectory: 262, total_reward 312.0\n",
      "trajectory: 263, total_reward 283.0\n",
      "trajectory: 264, total_reward 358.0\n",
      "trajectory: 265, total_reward 311.0\n",
      "trajectory: 266, total_reward 324.0\n",
      "trajectory: 267, total_reward 423.0\n",
      "trajectory: 268, total_reward 322.0\n",
      "trajectory: 269, total_reward 363.0\n",
      "trajectory: 270, total_reward 308.0\n",
      "trajectory: 271, total_reward 439.0\n",
      "trajectory: 272, total_reward 323.0\n",
      "trajectory: 273, total_reward 257.0\n",
      "trajectory: 274, total_reward 363.0\n",
      "trajectory: 275, total_reward 500.0\n",
      "trajectory: 276, total_reward 389.0\n",
      "trajectory: 277, total_reward 494.0\n",
      "trajectory: 278, total_reward 500.0\n",
      "trajectory: 279, total_reward 359.0\n",
      "trajectory: 280, total_reward 387.0\n",
      "trajectory: 281, total_reward 275.0\n",
      "trajectory: 282, total_reward 500.0\n",
      "trajectory: 283, total_reward 417.0\n",
      "trajectory: 284, total_reward 500.0\n",
      "trajectory: 285, total_reward 374.0\n",
      "trajectory: 286, total_reward 414.0\n",
      "trajectory: 287, total_reward 500.0\n",
      "trajectory: 288, total_reward 295.0\n",
      "trajectory: 289, total_reward 310.0\n",
      "trajectory: 290, total_reward 323.0\n",
      "trajectory: 291, total_reward 435.0\n",
      "trajectory: 292, total_reward 400.0\n",
      "trajectory: 293, total_reward 400.0\n",
      "trajectory: 294, total_reward 500.0\n",
      "trajectory: 295, total_reward 375.0\n",
      "trajectory: 296, total_reward 363.0\n",
      "trajectory: 297, total_reward 500.0\n",
      "trajectory: 298, total_reward 335.0\n",
      "trajectory: 299, total_reward 414.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_n = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_n, epsilon_decrease=1e-4)\n",
    "\n",
    "trajectory_n = 300\n",
    "trajectory_len = 500\n",
    "\n",
    "for trajectory in range(trajectory_n):\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    for t in range(trajectory_len):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        agent.fit(state, action, reward, done, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f'trajectory: {trajectory}, total_reward {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
